#!/usr/bin/env python3
"""
Ù…Ø­Ø±Ùƒ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ - Code Generation Engine
==========================================================
ÙŠØ³ØªØ·ÙŠØ¹: ØªÙˆÙ„ÙŠØ¯ ÙƒÙˆØ¯ØŒ Ø¥ØµÙ„Ø§Ø­ Ø£Ø®Ø·Ø§Ø¡ØŒ ØªØ­Ø³ÙŠÙ†ØŒ Ø´Ø±Ø­ØŒ ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠÙ† Ø§Ù„Ù„ØºØ§Øª
"""

import sys
import json
import ast
import re
import subprocess
import tempfile
import os
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ø­Ù„Ù„
from .code_analyzer import SmartCodeAnalyzer, CodeQualityAnalyzer

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Transformers
try:
    import torch
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, 
        AutoModelForSeq2SeqLM, pipeline
    )
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("âš ï¸  Transformers ØºÙŠØ± Ù…Ø«Ø¨Øª. Ø§Ø³ØªØ®Ø¯Ù…: pip install transformers torch")
    TRANSFORMERS_AVAILABLE = False


@dataclass
class CodeGenerationRequest:
    """Ø·Ù„Ø¨ ØªÙˆÙ„ÙŠØ¯ ÙƒÙˆØ¯ Ù…Ù†Ø¸Ù…"""
    description: str
    language: str = "python"
    code_type: str = "function"  # function, class, script, module
    input_signature: Optional[str] = None
    output_signature: Optional[str] = None
    constraints: List[str] = field(default_factory=list)
    test_cases: List[Dict] = field(default_factory=list)
    max_length: int = 512
    temperature: float = 0.7


@dataclass
class GenerationResult:
    """Ù†ØªÙŠØ¬Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯"""
    status: str
    generated_code: str
    language: str
    quality_score: float
    issues: List[Dict]
    test_cases: List[Dict]
    explanation: str
    execution_time_ms: float


class CodeGenEngine:
    """
    Ù…Ø­Ø±Ùƒ ØªÙˆÙ„ÙŠØ¯ ÙƒÙˆØ¯ Ù…ØªØ·ÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Transformers
    ÙŠØ¯Ø¹Ù…: CodeGen, CodeT5, GPT-4ALL, StarCoder
    """
    
    # Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¯Ø¹ÙˆÙ…Ø©
    SUPPORTED_MODELS = {
        "codegen-small": "Salesforce/codegen-350M-mono",
        "codegen-medium": "Salesforce/codegen-2B-mono",
        "codet5": "Salesforce/codet5-base",
        "starcoder": "bigcode/starcoder",
        "incoder": "facebook/incoder-1B"
    }
    
    def __init__(self, model_name: str = "Salesforce/codegen-350M-mono", 
                 use_cache: bool = True):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ø±Ùƒ
        
        Args:
            model_name: Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ùˆ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ù† SUPPORTED_MODELS
            use_cache: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸  Ø§Ù„Ø¬Ù‡Ø§Ø²: {self.device}")
        
        # Ø­Ù„ Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        if model_name in self.SUPPORTED_MODELS:
            model_name = self.SUPPORTED_MODELS[model_name]
        
        self.model_name = model_name
        self.use_cache = use_cache
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        
        if TRANSFORMERS_AVAILABLE:
            self._load_model()
        
        # Ø§Ù„Ù…Ø­Ù„Ù„ÙˆÙ†
        self.code_analyzer = CodeQualityAnalyzer()
        self.smart_analyzer = SmartCodeAnalyzer()
        
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ¹Ù„Ù…
        self.generation_history: List[Dict] = []
        self.cache: Dict[str, Any] = {}
        
        print("âœ… Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¬Ø§Ù‡Ø²")
    
    def _load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø­Ù„Ù„"""
        try:
            print(f"ğŸ¤– ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ù‡Ø§Ø²
            load_kwargs = {}
            if self.device.type == "cuda":
                load_kwargs = {
                    "torch_dtype": torch.float16,
                    "device_map": "auto"
                }
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **load_kwargs
            )
            
            # Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if self.device.type == "cuda" else -1
            )
            
            print("âœ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ù…Ù„ Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            print("ğŸ“ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¶Ø¹ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨")
    
    # ========== Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ==========
    
    def generate_code(self, request: CodeGenerationRequest) -> Dict[str, Any]:
        """
        ØªÙˆÙ„ÙŠØ¯ ÙƒÙˆØ¯ Ø°ÙƒÙŠ Ù…Ù† Ø§Ù„ÙˆØµÙ
        
        Args:
            request: Ø·Ù„Ø¨ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
            
        Returns:
            Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„
        """
        start_time = datetime.now()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒØ§Ø´
        cache_key = self._generate_cache_key(request)
        if self.use_cache and cache_key in self.cache:
            print("ğŸ“¦ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ØªÙŠØ¬Ø© Ù…Ø®Ø²Ù†Ø©")
            return self.cache[cache_key]
        
        # Ø¨Ù†Ø§Ø¡ prompt Ù…Ø­Ø³Ù‘Ù†
        prompt = self._build_enhanced_prompt(request)
        
        # Ø§Ù„ØªÙˆÙ„ÙŠØ¯
        if self.pipeline:
            generated_codes = self._generate_with_model(prompt, request)
        else:
            generated_codes = self._generate_with_templates(request)
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ø³Ø®Ø©
        best_code = self._select_best_candidate(generated_codes, request)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬ÙˆØ¯Ø©
        quality_report = self.code_analyzer.analyze(best_code)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        tests = self._auto_generate_tests(best_code, request)
        
        # Ø´Ø±Ø­ Ø§Ù„ÙƒÙˆØ¯
        explanation = self._explain_code(best_code)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª
        execution_time = (datetime.now() - start_time).total_seconds() * 1000
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        result = {
            "status": "success",
            "language": request.language,
            "generated_code": best_code,
            "quality_score": quality_report["overall_score"],
            "quality_details": quality_report,
            "issues": quality_report["issues"],
            "test_cases": tests,
            "explanation": explanation,
            "execution_time_ms": execution_time,
            "timestamp": datetime.now().isoformat()
        }
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ ÙˆØ§Ù„ØªØ§Ø±ÙŠØ®
        if self.use_cache:
            self.cache[cache_key] = result
        
        self.generation_history.append({
            "timestamp": datetime.now().isoformat(),
            "request": request.__dict__,
            "result": result
        })
        
        return result
    
    def _generate_with_model(self, prompt: str, 
                             request: CodeGenerationRequest) -> List[str]:
        """Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        print(f"ğŸ“ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯...")
        
        generation_config = {
            "max_length": request.max_length,
            "temperature": request.temperature,
            "top_p": 0.95,
            "top_k": 50,
            "do_sample": True,
            "num_return_sequences": 3,
            "pad_token_id": self.tokenizer.eos_token_id
        }
        
        try:
            sequences = self.pipeline(prompt, **generation_config)
            
            candidates = []
            for seq in sequences:
                generated_text = seq['generated_text']
                code = self._extract_code_block(generated_text)
                if code:
                    candidates.append(code)
            
            return candidates if candidates else [self._fallback_generation(request)]
            
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {e}")
            return [self._fallback_generation(request)]
    
    def _generate_with_templates(self, request: CodeGenerationRequest) -> List[str]:
        """Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ (ÙˆØ¶Ø¹ Ø¨Ø¯ÙˆÙ† Ù†Ù…ÙˆØ°Ø¬)"""
        templates = self._get_templates_for_type(request.code_type, request.language)
        
        candidates = []
        for template in templates:
            filled = self._fill_template(template, request)
            candidates.append(filled)
        
        return candidates
    
    def _get_templates_for_type(self, code_type: str, language: str) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚ÙˆØ§Ù„Ø¨ Ù„Ù„Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­Ø¯Ø¯"""
        templates = {
            "python": {
                "function": [
                    "def {name}({params}):\n    \"\"\"{description}\"\"\"\n    {body}\n    return result",
                    "def {name}({params}):\n    # {description}\n    {body}\n    return None"
                ],
                "class": [
                    "class {name}:\n    \"\"\"{description}\"\"\"\n    \n    def __init__(self):\n        pass"
                ]
            }
        }
        
        lang_templates = templates.get(language, templates["python"])
        return lang_templates.get(code_type, lang_templates["function"])
    
    def _fill_template(self, template: str, request: CodeGenerationRequest) -> str:
        """Ù…Ù„Ø¡ Ø§Ù„Ù‚Ø§Ù„Ø¨"""
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ù…Ù† Ø§Ù„ÙˆØµÙ
        name_match = re.search(r'(?:function|class|def)\s+(\w+)', request.description, re.I)
        name = name_match.group(1) if name_match else "generated_function"
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        params = request.input_signature if request.input_signature else "*args, **kwargs"
        
        return template.format(
            name=name,
            params=params,
            description=request.description,
            body="# TODO: Implement"
        )
    
    def _fallback_generation(self, request: CodeGenerationRequest) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ø­ØªÙŠØ§Ø·ÙŠ"""
        return f"""# {request.description}
def generated_function({request.input_signature or 'x'}):
    \"\"\"TODO: Implement this function\"\"\"
    # Auto-generated code
    pass
"""
    
    def _build_enhanced_prompt(self, request: CodeGenerationRequest) -> str:
        """Ø¨Ù†Ø§Ø¡ prompt Ù…ØªØ·ÙˆØ± ÙˆÙ…Ù†Ø¸Ù…"""
        prompt_parts = [
            f"# Generate {request.code_type} in {request.language}",
            f"# Description: {request.description}",
        ]
        
        if request.input_signature:
            prompt_parts.append(f"# Input: {request.input_signature}")
        
        if request.output_signature:
            prompt_parts.append(f"# Output: {request.output_signature}")
        
        if request.constraints:
            prompt_parts.append(f"# Constraints: {', '.join(request.constraints)}")
        
        if request.test_cases:
            prompt_parts.append("# Test Cases:")
            for test in request.test_cases[:2]:
                prompt_parts.append(f"# - Input: {test.get('input', 'N/A')} -> Expected: {test.get('expected', 'N/A')}")
        
        prompt_parts.extend([
            "",
            f"Here is the {request.language} {request.code_type}:",
            f"```{request.language}",
        ])
        
        return "\n".join(prompt_parts)
    
    def _extract_code_block(self, generated_text: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒØªÙ„Ø© Ø§Ù„ÙƒÙˆØ¯ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯"""
        # Ø§Ù„Ø¨Ø­Ø« Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…Ø§Øª ```
        patterns = [
            r'```python\n(.*?)```',
            r'```\n(.*?)```',
            r'<code>(.*?)</code>'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, generated_text, re.DOTALL)
            if match:
                return match.group(1).strip()
        
        # Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ØŒ Ø£Ø¹Ø¯ Ø§Ù„Ù†Øµ ÙƒØ§Ù…Ù„Ø§Ù‹
        lines = generated_text.split('\n')
        code_lines = []
        in_code = False
        
        for line in lines:
            if '```' in line:
                in_code = not in_code
                continue
            if in_code or line.strip().startswith(('def ', 'class ', 'import ', 'from ')):
                code_lines.append(line)
        
        return '\n'.join(code_lines) if code_lines else generated_text.strip()
    
    def _select_best_candidate(self, candidates: List[str], 
                               request: CodeGenerationRequest) -> str:
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ø³Ø®Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª"""
        if len(candidates) == 1:
            return candidates[0]
        
        scores = []
        
        for code in candidates:
            score = 0
            
            # ÙŠØªØ¨Ø¹ Ø§Ù„ØªÙˆÙ‚ÙŠØ¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ØŸ
            if request.input_signature and request.input_signature in code:
                score += 10
            
            # ÙŠØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø£Ù…Ø«Ù„ØŸ
            line_count = len(code.split('\n'))
            if 5 < line_count < 50:
                score += 5
            
            # ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„ÙˆØµÙØŸ
            keywords = request.description.lower().split()
            code_lower = code.lower()
            score += sum(2 for kw in keywords if len(kw) > 3 and kw in code_lower)
            
            # ØµÙŠØ§ØºØ© ØµØ­ÙŠØ­Ø©ØŸ
            if self._validate_syntax(code, request.language):
                score += 15
            
            # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù†Ø§Ø¬Ø­Ø©ØŸ
            if request.test_cases:
                if self._code_passes_tests(code, request.test_cases, request.language):
                    score += 20
            
            scores.append(score)
        
        best_idx = scores.index(max(scores))
        return candidates[best_idx]
    
    def _validate_syntax(self, code: str, language: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ØµÙŠØ§ØºØ©"""
        if language == "python":
            try:
                ast.parse(code)
                return True
            except SyntaxError:
                return False
        return True  # Ù„Ù„ØºØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰
    
    def _code_passes_tests(self, code: str, test_cases: List[Dict], 
                           language: str) -> bool:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ Ù„Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ÙˆÙ„Ø¯"""
        if language != "python":
            return True
        
        try:
            namespace = {}
            exec(code, namespace)
            
            for test in test_cases[:1]:  # Ø£ÙˆÙ„ Ø§Ø®ØªØ¨Ø§Ø± ÙÙ‚Ø·
                func_name = test.get('function_name', 'generated_function')
                func = namespace.get(func_name)
                
                if func:
                    test_input = test.get('input', [])
                    expected = test.get('expected')
                    
                    if isinstance(test_input, list):
                        result = func(*test_input)
                    else:
                        result = func(test_input)
                    
                    return result == expected
            
            return True
        except Exception as e:
            return False
    
    def _auto_generate_tests(self, code: str, 
                             request: CodeGenerationRequest) -> List[Dict]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ©"""
        tests = []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ø¯Ø§Ù„Ø©
        func_match = re.search(r'def\s+(\w+)\s*\(', code)
        if func_match:
            func_name = func_match.group(1)
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ·
            tests.append({
                "function_name": func_name,
                "input": [],
                "expected": None,
                "description": "Test basic execution"
            })
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ Ù…Ø¹Ø§Ù…Ù„Ø§Øª
            if request.input_signature:
                tests.append({
                    "function_name": func_name,
                    "input": [1, 2, 3],
                    "expected": None,
                    "description": "Test with parameters"
                })
        
        return tests
    
    def _explain_code(self, code: str) -> str:
        """Ø´Ø±Ø­ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ÙˆÙ„Ø¯"""
        lines = code.split('\n')
        explanation = []
        
        # Ø´Ø±Ø­ Ø§Ù„Ø¯ÙˆØ§Ù„
        for i, line in enumerate(lines):
            if line.strip().startswith('def '):
                func_name = line.split('def ')[1].split('(')[0]
                explanation.append(f"- Ø§Ù„Ø¯Ø§Ù„Ø© '{func_name}' ØªÙ‚ÙˆÙ… Ø¨Ù€...")
            elif line.strip().startswith('class '):
                class_name = line.split('class ')[1].split(':')[0].split('(')[0]
                explanation.append(f"- Ø§Ù„ÙØ¦Ø© '{class_name}' ØªØ¹Ø±Ù...")
        
        return "\n".join(explanation) if explanation else "ÙƒÙˆØ¯ Ø¨Ø³ÙŠØ·"
    
    def _generate_cache_key(self, request: CodeGenerationRequest) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ Ù„Ù„ÙƒØ§Ø´"""
        key_data = f"{request.description}:{request.language}:{request.code_type}"
        import hashlib
        return hashlib.md5(key_data.encode()).hexdigest()
    
    # ========== Ø§Ù„Ø¥ØµÙ„Ø§Ø­ ÙˆØ§Ù„ØªØ­Ø³ÙŠÙ† ==========
    
    def fix_code(self, broken_code: str, error_message: str = "") -> Dict[str, Any]:
        """Ø¥ØµÙ„Ø§Ø­ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ÙƒÙˆØ¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
        language = self.smart_analyzer._detect_language(broken_code)
        
        prompt = f"""# Fix this {language} code
# Error: {error_message}
# Original Code:
```{language}
{broken_code}
```
# Fixed Code:
```{language}
"""
        
        if self.pipeline:
            try:
                result = self.pipeline(
                    prompt,
                    max_length=512,
                    temperature=0.3,
                    do_sample=False
                )[0]['generated_text']
                
                fixed_code = self._extract_code_block(result)
            except:
                fixed_code = self._manual_fix(broken_code, error_message)
        else:
            fixed_code = self._manual_fix(broken_code, error_message)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥ØµÙ„Ø§Ø­
        is_valid = self._validate_syntax(fixed_code, language)
        
        return {
            "status": "success" if is_valid else "partial",
            "original_code": broken_code,
            "fixed_code": fixed_code,
            "is_valid": is_valid,
            "language": language,
            "improvements": self._compare_codes(broken_code, fixed_code),
            "timestamp": datetime.now().isoformat()
        }
    
    def _manual_fix(self, code: str, error_message: str) -> str:
        """Ø¥ØµÙ„Ø§Ø­ ÙŠØ¯ÙˆÙŠ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©"""
        fixed = code
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ù‚ÙˆØ§Ø³ ØºÙŠØ± Ø§Ù„Ù…ØºÙ„Ù‚Ø©
        open_parens = fixed.count('(') - fixed.count(')')
        if open_parens > 0:
            fixed += ')' * open_parens
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
        if ':' not in fixed and 'def ' in fixed:
            lines = fixed.split('\n')
            for i, line in enumerate(lines):
                if line.strip().startswith('def ') or line.strip().startswith('class '):
                    if not line.rstrip().endswith(':'):
                        lines[i] = line.rstrip() + ':'
            fixed = '\n'.join(lines)
        
        return fixed
    
    def optimize_code(self, code: str, 
                      optimization_type: str = "performance") -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙƒÙˆØ¯ Ù„Ù„Ø£Ø¯Ø§Ø¡"""
        language = self.smart_analyzer._detect_language(code)
        
        prompt = f"""# Optimize this {language} code for {optimization_type}
# Original Code:
```{language}
{code}
```
# Optimized Code:
```{language}
"""
        
        if self.pipeline:
            try:
                result = self.pipeline(
                    prompt,
                    max_length=512,
                    temperature=0.2,
                    do_sample=True
                )[0]['generated_text']
                
                optimized_code = self._extract_code_block(result)
            except:
                optimized_code = self._manual_optimize(code, optimization_type)
        else:
            optimized_code = self._manual_optimize(code, optimization_type)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        original_analysis = self.code_analyzer.analyze(code)
        optimized_analysis = self.code_analyzer.analyze(optimized_code)
        
        performance_gain = self._estimate_performance_gain(
            original_analysis, optimized_analysis
        )
        
        return {
            "status": "success",
            "original_code": code,
            "optimized_code": optimized_code,
            "performance_improvement": performance_gain,
            "changes": self._extract_changes(code, optimized_code),
            "original_score": original_analysis["overall_score"],
            "optimized_score": optimized_analysis["overall_score"],
            "timestamp": datetime.now().isoformat()
        }
    
    def _manual_optimize(self, code: str, optimization_type: str) -> str:
        """ØªØ­Ø³ÙŠÙ† ÙŠØ¯ÙˆÙŠ"""
        optimized = code
        
        if optimization_type == "performance":
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø¥Ù„Ù‰ list comprehension
            optimized = re.sub(
                r'result\s*=\s*\[\]\s*\nfor\s+(\w+)\s+in\s+(\w+):\s*\n\s*result\.append\(([^)]+)\)',
                r'result = [\3 for \1 in \2]',
                optimized
            )
        
        return optimized
    
    def _estimate_performance_gain(self, original: Dict, optimized: Dict) -> str:
        """ØªÙ‚Ø¯ÙŠØ± ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        orig_complexity = original["metrics"]["complexity"]
        opt_complexity = optimized["metrics"]["complexity"]
        
        if orig_complexity > 0:
            improvement = (orig_complexity - opt_complexity) / orig_complexity * 100
            return f"{improvement:.1f}% faster"
        
        return "Unknown improvement"
    
    def _compare_codes(self, original: str, modified: str) -> List[str]:
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ù†Ø³Ø®ØªÙŠÙ† Ù…Ù† Ø§Ù„ÙƒÙˆØ¯"""
        improvements = []
        
        orig_lines = len(original.split('\n'))
        mod_lines = len(modified.split('\n'))
        
        if mod_lines < orig_lines:
            improvements.append(f"Reduced from {orig_lines} to {mod_lines} lines")
        
        return improvements
    
    def _extract_changes(self, original: str, modified: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª"""
        changes = []
        
        orig_set = set(original.split('\n'))
        mod_set = set(modified.split('\n'))
        
        added = mod_set - orig_set
        removed = orig_set - mod_set
        
        if added:
            changes.append(f"Added {len(added)} lines")
        if removed:
            changes.append(f"Removed {len(removed)} lines")
        
        return changes
    
    # ========== Ø§Ù„ØªØ±Ø¬Ù…Ø© ÙˆØ§Ù„ØªÙˆØ«ÙŠÙ‚ ==========
    
    def translate_code(self, code: str, target_language: str) -> Dict[str, Any]:
        """ØªØ±Ø¬Ù…Ø© Ø§Ù„ÙƒÙˆØ¯ Ø¨ÙŠÙ† Ø§Ù„Ù„ØºØ§Øª"""
        source_language = self.smart_analyzer._detect_language(code)
        
        prompt = f"""# Translate from {source_language} to {target_language}
# Original Code:
```{source_language}
{code}
```
# Translated Code:
```{target_language}
"""
        
        if self.pipeline:
            try:
                result = self.pipeline(
                    prompt,
                    max_length=512,
                    temperature=0.3
                )[0]['generated_text']
                
                translated_code = self._extract_code_block(result)
            except:
                translated_code = f"# Translation to {target_language}\n# TODO: Implement"
        else:
            translated_code = self._manual_translate(code, source_language, target_language)
        
        return {
            "status": "success",
            "source_language": source_language,
            "target_language": target_language,
            "original_code": code,
            "translated_code": translated_code,
            "timestamp": datetime.now().isoformat()
        }
    
    def _manual_translate(self, code: str, source: str, target: str) -> str:
        """ØªØ±Ø¬Ù…Ø© ÙŠØ¯ÙˆÙŠØ© Ø¨Ø³ÙŠØ·Ø©"""
        translations = {
            ("python", "javascript"): {
                "def ": "function ",
                "None": "null",
                "True": "true",
                "False": "false",
                "# ": "// "
            }
        }
        
        translated = code
        trans_map = translations.get((source, target), {})
        
        for old, new in trans_map.items():
            translated = translated.replace(old, new)
        
        return translated
    
    def generate_documentation(self, code: str) -> Dict[str, Any]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØ«ÙŠÙ‚ Ù„Ù„ÙƒÙˆØ¯"""
        analysis = self.smart_analyzer.analyze(code)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¯ÙˆØ§Ù„ ÙˆØ§Ù„ÙØ¦Ø§Øª
        tree = ast.parse(code)
        
        docs = {
            "summary": f"Code with {analysis['metrics']['function_count']} functions and {analysis['metrics']['class_count']} classes",
            "functions": [],
            "classes": [],
            "quality": analysis['quality_score']
        }
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                docs["functions"].append({
                    "name": node.name,
                    "line": node.lineno,
                    "docstring": ast.get_docstring(node) or "No documentation"
                })
            elif isinstance(node, ast.ClassDef):
                docs["classes"].append({
                    "name": node.name,
                    "line": node.lineno,
                    "docstring": ast.get_docstring(node) or "No documentation"
                })
        
        return docs
    
    # ========== Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ==========
    
    def get_history(self) -> List[Dict]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙˆÙ„ÙŠØ¯"""
        return self.generation_history
    
    def clear_cache(self):
        """Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ø´"""
        self.cache.clear()
        print("ğŸ—‘ï¸  Cache cleared")
    
    def save_history(self, filepath: str):
        """Ø­ÙØ¸ Ø§Ù„ØªØ§Ø±ÙŠØ®"""
        with open(filepath, 'w') as f:
            json.dump(self.generation_history, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ History saved to {filepath}")


# Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹
if __name__ == "__main__":
    engine = CodeGenEngine()
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯
    request = CodeGenerationRequest(
        description="Calculate factorial of a number",
        language="python",
        code_type="function",
        input_signature="n: int",
        output_signature="int",
        test_cases=[{"input": [5], "expected": 120}]
    )
    
    result = engine.generate_code(request)
    
    print("\n" + "=" * 60)
    print("ğŸ¯ Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯")
    print("=" * 60)
    print(f"\nğŸ“Š Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬ÙˆØ¯Ø©: {result['quality_score']:.1f}/100")
    print(f"â±ï¸  ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°: {result['execution_time_ms']:.1f}ms")
    print(f"\nğŸ“ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ÙˆÙ„Ø¯:")
    print("-" * 40)
    print(result['generated_code'])
    print("-" * 40)
    print(f"\nğŸ“– Ø§Ù„Ø´Ø±Ø­:\n{result['explanation']}")
